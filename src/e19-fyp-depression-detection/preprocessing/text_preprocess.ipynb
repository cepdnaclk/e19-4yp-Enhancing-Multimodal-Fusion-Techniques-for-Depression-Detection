{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01827a4a-3ce3-4667-9fcd-e2c500dea8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transcript for 300_P:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.3</td>\n",
       "      <td>15.1</td>\n",
       "      <td>so I'm going to</td>\n",
       "      <td>0.934210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>interview in Spanish</td>\n",
       "      <td>0.608470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.9</td>\n",
       "      <td>24.3</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.690606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62.1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>good</td>\n",
       "      <td>0.951897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>Atlanta Georgia</td>\n",
       "      <td>0.987629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start_Time  End_Time                   Text  Confidence\n",
       "0        14.3      15.1        so I'm going to    0.934210\n",
       "1        20.3      21.1   interview in Spanish    0.608470\n",
       "2        23.9      24.3                   okay    0.690606\n",
       "3        62.1      62.7                   good    0.951897\n",
       "4        68.8      69.8        Atlanta Georgia    0.987629"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transcript for 301_P:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>yeah there's also on Craigslist so that's why</td>\n",
       "      <td>0.883057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.9</td>\n",
       "      <td>42.5</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.960925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.9</td>\n",
       "      <td>55.8</td>\n",
       "      <td>how are you doing today I'm doing good thank you</td>\n",
       "      <td>0.950963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.7</td>\n",
       "      <td>60.7</td>\n",
       "      <td>I'm from Los Angeles</td>\n",
       "      <td>0.970176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.4</td>\n",
       "      <td>64.2</td>\n",
       "      <td>I'm great</td>\n",
       "      <td>0.904099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start_Time  End_Time                                               Text  \\\n",
       "0         0.8       7.0      yeah there's also on Craigslist so that's why   \n",
       "1        41.9      42.5                                               okay   \n",
       "2        52.9      55.8   how are you doing today I'm doing good thank you   \n",
       "3        59.7      60.7                               I'm from Los Angeles   \n",
       "4        63.4      64.2                                          I'm great   \n",
       "\n",
       "   Confidence  \n",
       "0    0.883057  \n",
       "1    0.960925  \n",
       "2    0.950963  \n",
       "3    0.970176  \n",
       "4    0.904099  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transcript for 302_P:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>just move around a little bit</td>\n",
       "      <td>0.906568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.3</td>\n",
       "      <td>27.1</td>\n",
       "      <td>when you're finished</td>\n",
       "      <td>0.793796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.2</td>\n",
       "      <td>59.1</td>\n",
       "      <td>how are you doing today</td>\n",
       "      <td>0.859790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>I'm fine how about yourself</td>\n",
       "      <td>0.987629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.6</td>\n",
       "      <td>67.3</td>\n",
       "      <td>where you from</td>\n",
       "      <td>0.911304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start_Time  End_Time                           Text  Confidence\n",
       "0         2.1       3.2  just move around a little bit    0.906568\n",
       "1        26.3      27.1           when you're finished    0.793796\n",
       "2        58.2      59.1        how are you doing today    0.859790\n",
       "3        59.6      61.0    I'm fine how about yourself    0.987629\n",
       "4        66.6      67.3                 where you from    0.911304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transcript for 308_P:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>okay perfect so we just want to move around a ...</td>\n",
       "      <td>0.927091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.7</td>\n",
       "      <td>13.6</td>\n",
       "      <td>all right now you got perfect</td>\n",
       "      <td>0.758649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>okay</td>\n",
       "      <td>0.901707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>good to go now</td>\n",
       "      <td>0.755682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>when she's done talking to you when you go ah...</td>\n",
       "      <td>0.891598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Start_Time  End_Time                                               Text  \\\n",
       "0         0.0       9.6  okay perfect so we just want to move around a ...   \n",
       "1        11.7      13.6                      all right now you got perfect   \n",
       "2        17.4      18.0                                               okay   \n",
       "3        20.0      21.0                                     good to go now   \n",
       "4        24.9      31.1   when she's done talking to you when you go ah...   \n",
       "\n",
       "   Confidence  \n",
       "0    0.927091  \n",
       "1    0.758649  \n",
       "2    0.901707  \n",
       "3    0.755682  \n",
       "4    0.891598  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Base path relative to text_preprocess.ipynb\n",
    "data_dir = os.path.join('..', 'data')\n",
    "\n",
    "# List all subject folders that end with _P\n",
    "subject_folders = [\n",
    "    folder for folder in os.listdir(data_dir)\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)) and folder.endswith('_P')\n",
    "]\n",
    "\n",
    "for subject in sorted(subject_folders):\n",
    "    subject_id = subject.split('_')[0]  # e.g., '300' from '300_P'\n",
    "    transcript_path = os.path.join(data_dir, subject, 'text', f'{subject_id}_Transcript.csv')\n",
    "\n",
    "    if os.path.exists(transcript_path):\n",
    "        print(f\"\\n✅ Transcript for {subject}:\\n\")\n",
    "        df = pd.read_csv(transcript_path)\n",
    "        display(df.head())  # or use print(df.head()) if not in Jupyter\n",
    "    else:\n",
    "        print(f\"❌ Transcript file not found for {subject} at path: {transcript_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e34c2fd-2106-4424-ada4-281933494c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sachithdissanayaka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sachithdissanayaka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing 300_Transcript.csv for 300_P...\n",
      "✅ Saved to /Users/sachithdissanayaka/Documents/FYP/e19-fyp-depression-detection/data/300_P/text/300_Transcript_processed_scaled.csv\n",
      "📊 Final feature shape: (68, 52)\n",
      "🔍 Processing 302_Transcript.csv for 302_P...\n",
      "✅ Saved to /Users/sachithdissanayaka/Documents/FYP/e19-fyp-depression-detection/data/302_P/text/302_Transcript_processed_scaled.csv\n",
      "📊 Final feature shape: (94, 52)\n",
      "🔍 Processing 308_Transcript.csv for 308_P...\n",
      "✅ Saved to /Users/sachithdissanayaka/Documents/FYP/e19-fyp-depression-detection/data/308_P/text/308_Transcript_processed_scaled.csv\n",
      "📊 Final feature shape: (144, 52)\n",
      "🔍 Processing 301_Transcript.csv for 301_P...\n",
      "✅ Saved to /Users/sachithdissanayaka/Documents/FYP/e19-fyp-depression-detection/data/301_P/text/301_Transcript_processed_scaled.csv\n",
      "📊 Final feature shape: (69, 52)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script performs feature extraction from transcript CSV files for a multimodal depression detection system.\n",
    "\n",
    "✔️ It reads subject-wise transcript files from the `text` folder.\n",
    "✔️ Cleans and tokenizes each spoken text entry.\n",
    "✔️ Extracts features using:\n",
    "    - Word2Vec (semantic vector representations)\n",
    "    - TF-IDF (term frequency-inverse document frequency)\n",
    "    - N-grams (bi-grams and tri-grams)\n",
    "✔️ Combines and reduces dimensionality using TruncatedSVD.\n",
    "✔️ Applies StandardScaler to normalize the features.\n",
    "✔️ Keeps the original Start_Time and End_Time for each entry.\n",
    "✔️ Saves the final feature set (with timestamps) as a new CSV file \n",
    "   in the same `text` folder under the name: \n",
    "   `<original_filename>_processed_scaled.csv`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Constants\n",
    "BASE_DIR = \"../data\"\n",
    "VECTOR_SIZE = 100\n",
    "SVD_COMPONENTS = 50  # Target final feature size\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except LookupError:\n",
    "        print(\"⚠️ Falling back to .split() due to NLTK error.\")\n",
    "        tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english') and len(w) > 1]\n",
    "    return tokens\n",
    "\n",
    "# Word2Vec vectorization\n",
    "def get_word2vec_features(tokens, model, vector_size=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not vectors:\n",
    "        return [0] * vector_size\n",
    "    return np.mean(vectors, axis=0).tolist()\n",
    "\n",
    "# Iterate through all subject folders\n",
    "for subject in os.listdir(BASE_DIR):\n",
    "    if not subject.endswith(\"_P\"):\n",
    "        continue\n",
    "\n",
    "    text_dir = os.path.join(BASE_DIR, subject, \"text\")\n",
    "    if not os.path.exists(text_dir):\n",
    "        print(f\"🚫 Skipping {subject}, no text directory.\")\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(text_dir):\n",
    "        if not file.endswith(\"_Transcript.csv\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(text_dir, file)\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        col = 'Text' if 'Text' in df.columns else df.columns[2]\n",
    "        df['clean_text'] = df[col].apply(clean_text)\n",
    "        df = df[df['clean_text'].map(len) > 0]\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"⚠️ Skipping {subject}, cleaned data is empty.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"🔍 Processing {file} for {subject}...\")\n",
    "\n",
    "        # Train Word2Vec\n",
    "        sentences = df['clean_text'].tolist()\n",
    "        w2v_model = Word2Vec(sentences=sentences, vector_size=VECTOR_SIZE, window=5, min_count=1, workers=4)\n",
    "        w2v_features = df['clean_text'].apply(lambda x: get_word2vec_features(x, w2v_model, VECTOR_SIZE))\n",
    "        w2v_df = pd.DataFrame(w2v_features.tolist(), columns=[f\"w2v_{i}\" for i in range(VECTOR_SIZE)])\n",
    "\n",
    "        # TF-IDF features\n",
    "        tfidf = TfidfVectorizer(max_features=500)\n",
    "        tfidf_matrix = tfidf.fit_transform(df['clean_text'].apply(lambda x: \" \".join(x)))\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "        # N-gram features (bi-grams and tri-grams)\n",
    "        ngram = CountVectorizer(ngram_range=(2, 3), max_features=200)\n",
    "        ngram_matrix = ngram.fit_transform(df['clean_text'].apply(lambda x: \" \".join(x)))\n",
    "        ngram_df = pd.DataFrame(ngram_matrix.toarray(), columns=ngram.get_feature_names_out())\n",
    "\n",
    "        # Combine all feature sets\n",
    "        all_features = pd.concat([w2v_df, tfidf_df, ngram_df], axis=1)\n",
    "\n",
    "        # Dimensionality reduction using TruncatedSVD\n",
    "        reduced_components = min(SVD_COMPONENTS, all_features.shape[1])\n",
    "        svd = TruncatedSVD(n_components=reduced_components)\n",
    "        svd_features = svd.fit_transform(all_features)\n",
    "\n",
    "        # Optional: Zero-pad if reduced_components < 50\n",
    "        if reduced_components < SVD_COMPONENTS:\n",
    "            padding = np.zeros((svd_features.shape[0], SVD_COMPONENTS - reduced_components))\n",
    "            svd_features = np.hstack((svd_features, padding))\n",
    "\n",
    "        # Normalize the final feature set\n",
    "        scaled = StandardScaler().fit_transform(svd_features)\n",
    "\n",
    "        # Include Start_Time and End_Time in the final DataFrame\n",
    "        timestamp_df = df[['Start_Time', 'End_Time']].reset_index(drop=True)\n",
    "        final_df = pd.DataFrame(scaled, columns=[f\"feature_{i}\" for i in range(SVD_COMPONENTS)])\n",
    "        final_with_timestamps = pd.concat([timestamp_df, final_df], axis=1)\n",
    "\n",
    "        # Save processed features\n",
    "        output_filename = file.replace(\".csv\", \"_processed_scaled.csv\")\n",
    "        output_path = os.path.join(text_dir, output_filename)\n",
    "        final_with_timestamps.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"✅ Saved to {output_path}\")\n",
    "        print(f\"📊 Final feature shape: {final_with_timestamps.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0e2f6a-fdbd-4aeb-8bad-d10f6c1c67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Found text folder: ../data/300_P/text\n",
      "\n",
      "🔍 Found text CSV files: ['../data/300_P/text/300_Transcript_processed_scaled.csv']\n",
      "\n",
      "🔍 Processing 300_Transcript_processed_scaled.csv for 300_P...\n",
      "Initial shape: (68, 52)\n",
      "⚠️ No default_column_names provided, keeping original text feature column names.\n",
      "Scaled numeric text feature columns.\n",
      "✅ Saved to ../data/300_P/text/processed/300_Transcript_reprocessed_scaled.csv\n",
      "📊 Final feature shape: (68, 52)\n",
      "🚫 No text folder in: lables\n",
      "\n",
      "📂 Found text folder: ../data/302_P/text\n",
      "\n",
      "🔍 Found text CSV files: ['../data/302_P/text/302_Transcript_processed_scaled.csv']\n",
      "\n",
      "🔍 Processing 302_Transcript_processed_scaled.csv for 302_P...\n",
      "Initial shape: (94, 52)\n",
      "⚠️ No default_column_names provided, keeping original text feature column names.\n",
      "Scaled numeric text feature columns.\n",
      "✅ Saved to ../data/302_P/text/processed/302_Transcript_reprocessed_scaled.csv\n",
      "📊 Final feature shape: (94, 52)\n",
      "\n",
      "📂 Found text folder: ../data/308_P/text\n",
      "\n",
      "🔍 Found text CSV files: ['../data/308_P/text/308_Transcript_processed_scaled.csv']\n",
      "\n",
      "🔍 Processing 308_Transcript_processed_scaled.csv for 308_P...\n",
      "Initial shape: (144, 52)\n",
      "⚠️ No default_column_names provided, keeping original text feature column names.\n",
      "Scaled numeric text feature columns.\n",
      "✅ Saved to ../data/308_P/text/processed/308_Transcript_reprocessed_scaled.csv\n",
      "📊 Final feature shape: (144, 52)\n",
      "\n",
      "📂 Found text folder: ../data/301_P/text\n",
      "\n",
      "🔍 Found text CSV files: ['../data/301_P/text/301_Transcript_processed_scaled.csv']\n",
      "\n",
      "🔍 Processing 301_Transcript_processed_scaled.csv for 301_P...\n",
      "Initial shape: (69, 52)\n",
      "⚠️ No default_column_names provided, keeping original text feature column names.\n",
      "Scaled numeric text feature columns.\n",
      "✅ Saved to ../data/301_P/text/processed/301_Transcript_reprocessed_scaled.csv\n",
      "📊 Final feature shape: (69, 52)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "def preprocess_text(input_directory, output_directory, unwanted_columns=[], rename_dict={}, fill_value=0, default_column_names=None):\n",
    "    \"\"\"\n",
    "    Preprocess text CSV files:\n",
    "    - Finds files like '*_Transcript_processed_scaled.csv' in input_directory.\n",
    "    - Keeps timestamp columns unchanged (if present).\n",
    "    - Removes unwanted columns and columns containing 'unknown' or 'unknow'.\n",
    "    - Renames columns as per rename_dict or default_column_names.\n",
    "    - Fills missing values with fill_value.\n",
    "    - Scales numeric columns except timestamps.\n",
    "    - Saves processed data to output_directory with suffix '_reprocessed_scaled.csv'.\n",
    "    - Prints progress and shape info.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(input_directory):\n",
    "        print(f\"❌ Error: Input directory {input_directory} does not exist!\")\n",
    "        return\n",
    "\n",
    "    # Look for already processed CSV files for further processing\n",
    "    text_csv_files = glob.glob(os.path.join(input_directory, '*_Transcript_processed_scaled.csv'))\n",
    "\n",
    "    if not text_csv_files:\n",
    "        print(f\"⚠️ No '*_Transcript_processed_scaled.csv' files found in {input_directory}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🔍 Found text CSV files: {text_csv_files}\")\n",
    "\n",
    "    for text_file in text_csv_files:\n",
    "        filename = os.path.basename(text_file)\n",
    "        participant_id = filename.split('_')[0]  # e.g., '300' from '300_Transcript_processed_scaled.csv'\n",
    "\n",
    "        print(f\"\\n🔍 Processing {filename} for {participant_id}_P...\")\n",
    "\n",
    "        df = pd.read_csv(text_file)\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "        # Timestamp columns to keep unchanged if present\n",
    "        timestamp_cols = ['Start_Time', 'End_Time']\n",
    "        existing_timestamp_cols = [col for col in timestamp_cols if col in df.columns]\n",
    "\n",
    "        # Feature columns exclude timestamps\n",
    "        feature_cols = [col for col in df.columns if col not in existing_timestamp_cols]\n",
    "\n",
    "        # Rename text feature columns if default_column_names provided\n",
    "        if default_column_names is not None and len(default_column_names) == len(feature_cols):\n",
    "            rename_map = dict(zip(feature_cols, default_column_names))\n",
    "            df.rename(columns=rename_map, inplace=True)\n",
    "            print(f\"✅ Text feature columns renamed using default_column_names.\")\n",
    "        elif default_column_names is None:\n",
    "            print(f\"⚠️ No default_column_names provided, keeping original text feature column names.\")\n",
    "\n",
    "        # Remove unwanted columns if exist\n",
    "        cols_to_drop = [col for col in unwanted_columns if col in df.columns]\n",
    "        if cols_to_drop:\n",
    "            df.drop(columns=cols_to_drop, inplace=True)\n",
    "            print(f\"Removed unwanted columns: {cols_to_drop}\")\n",
    "\n",
    "        # Remove columns containing 'unknown' or 'unknow' anywhere\n",
    "        cols_with_unknowns = [col for col in feature_cols if col in df.columns and df[col].isin(['unknown', 'unknow']).any()]\n",
    "        if cols_with_unknowns:\n",
    "            df.drop(columns=cols_with_unknowns, inplace=True)\n",
    "            print(f\"Removed columns with 'unknown' values: {cols_with_unknowns}\")\n",
    "\n",
    "        # Rename columns according to rename_dict (excluding timestamps)\n",
    "        rename_dict_filtered = {k: v for k, v in rename_dict.items() if k in df.columns and k not in existing_timestamp_cols}\n",
    "        if rename_dict_filtered:\n",
    "            df.rename(columns=rename_dict_filtered, inplace=True)\n",
    "            print(f\"Renamed columns as per rename_dict: {rename_dict_filtered}\")\n",
    "\n",
    "        # Fill missing values\n",
    "        df.fillna(fill_value, inplace=True)\n",
    "\n",
    "        # Scale numeric feature columns (exclude timestamps)\n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.difference(existing_timestamp_cols)\n",
    "        if not numeric_cols.empty:\n",
    "            scaler = StandardScaler()\n",
    "            df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "            print(f\"Scaled numeric text feature columns.\")\n",
    "\n",
    "        # Prepare output directory\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Save with a new suffix to avoid overwriting original processed file\n",
    "        output_filename = filename.replace('_processed_scaled.csv', '_reprocessed_scaled.csv')\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"✅ Saved to {output_path}\")\n",
    "\n",
    "        # Print shape of saved file\n",
    "        saved_df = pd.read_csv(output_path)\n",
    "        print(f\"📊 Final feature shape: {saved_df.shape}\")\n",
    "\n",
    "\n",
    "def process_all_text_participants(root_data_dir):\n",
    "    \"\"\"\n",
    "    Processes text folders of all participants inside root_data_dir.\n",
    "    Skips hidden files/folders like .DS_Store or .ipynb_checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    for participant_folder in os.listdir(root_data_dir):\n",
    "        if participant_folder.startswith('.'):\n",
    "            continue  # skip hidden/system files and folders\n",
    "\n",
    "        participant_path = os.path.join(root_data_dir, participant_folder)\n",
    "        text_dir = os.path.join(participant_path, \"text\")\n",
    "\n",
    "        if os.path.isdir(text_dir):\n",
    "            print(f\"\\n📂 Found text folder: {text_dir}\")\n",
    "            output_dir = os.path.join(text_dir, \"processed\")\n",
    "            preprocess_text(\n",
    "                input_directory=text_dir,\n",
    "                output_directory=output_dir,\n",
    "                unwanted_columns=[\"unwanted_column\"],  # Customize if needed\n",
    "                rename_dict={\"old_name\": \"new_name\"},  # Customize if needed\n",
    "                fill_value=0,\n",
    "                default_column_names=None  # Provide a list if you want to rename features\n",
    "            )\n",
    "        else:\n",
    "            print(f\"🚫 No text folder in: {participant_folder}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_data_dir = \"../data\"\n",
    "    process_all_text_participants(root_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0483d36-e933-46bc-a32b-547a8ad0f295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
