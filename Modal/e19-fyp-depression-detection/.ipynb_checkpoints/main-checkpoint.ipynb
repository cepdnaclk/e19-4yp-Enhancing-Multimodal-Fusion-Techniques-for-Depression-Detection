{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6aa856-db3d-4026-96e0-257ff501cec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to set up the folder structure?  yes\n"
     ]
    }
   ],
   "source": [
    "# This script moves all folders ending with '_P' into a 'data' directory after user confirmation.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "user_input = input(\"Do you want to set up the folder structure? \").strip().lower()\n",
    "\n",
    "if user_input == 'yes':\n",
    "    base_dir = \".\"\n",
    "    destination_dir = os.path.join(base_dir, \"data\")\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for parent_folder in os.listdir(base_dir):\n",
    "        if parent_folder.endswith('_P') and os.path.isdir(os.path.join(base_dir, parent_folder)):\n",
    "            # Define the source and destination subfolders\n",
    "            source_subfolder = os.path.join(base_dir, parent_folder)\n",
    "            destination_subfolder = os.path.join(destination_dir, parent_folder)\n",
    "\n",
    "            # Step 2: Move the subfolder to the new location\n",
    "            if os.path.exists(source_subfolder):\n",
    "                shutil.move(source_subfolder, destination_subfolder)\n",
    "                print(f\"üìÇ Moved {source_subfolder} ‚Üí {destination_subfolder}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Subfolder not found: {source_subfolder}\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Delete the now-empty parent folder\n",
    "            parent_path = os.path.join(base_dir, parent_folder)\n",
    "            try:\n",
    "                shutil.rmtree(parent_path)\n",
    "                print(f\"üóëÔ∏è Deleted folder: {parent_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to delete {parent_path}: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Operation cancelled. Folder structure setup was not performed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deceb8b1-9531-46a8-966b-45b350fc6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to restructure the subject folders?  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/300_P\n",
      "\n",
      "üîÑ Processing: 300_P\n",
      "‚ö†Ô∏è Transcript not found: ./data/300_P/300_Transcript.csv\n",
      "‚ö†Ô∏è Features folder not found: ./data/300_P/features\n",
      "./data/.DS_Store\n",
      "‚ùå Skipping non-folder: ./data/.DS_Store\n",
      "./data/lables\n",
      "‚ùå Skipping non-subject folder: ./data/lables\n",
      "./data/302_P\n",
      "\n",
      "üîÑ Processing: 302_P\n",
      "‚ö†Ô∏è Transcript not found: ./data/302_P/302_Transcript.csv\n",
      "‚ö†Ô∏è Features folder not found: ./data/302_P/features\n",
      "./data/308_P\n",
      "\n",
      "üîÑ Processing: 308_P\n",
      "‚ö†Ô∏è Transcript not found: ./data/308_P/308_Transcript.csv\n",
      "‚ö†Ô∏è Features folder not found: ./data/308_P/features\n",
      "./data/aligned_multimodal_dataset.csv\n",
      "‚ùå Skipping non-folder: ./data/aligned_multimodal_dataset.csv\n",
      "./data/301_P\n",
      "\n",
      "üîÑ Processing: 301_P\n",
      "‚ö†Ô∏è Transcript not found: ./data/301_P/301_Transcript.csv\n",
      "‚ö†Ô∏è Features folder not found: ./data/301_P/features\n",
      "./data/.ipynb_checkpoints\n",
      "‚ùå Skipping non-subject folder: ./data/.ipynb_checkpoints\n",
      "\n",
      "‚úÖ Done restructuring each *_P folder.\n"
     ]
    }
   ],
   "source": [
    "# This script organizes features and transcript files into 'text', 'audio', 'video', and 'clinical' folders inside each *_P subject folder, after user confirmation.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Ask user for confirmation\n",
    "user_input = input(\"Do you want to restructure the subject folders? \").strip().lower()\n",
    "\n",
    "if user_input == 'yes':\n",
    "    # Set your base directory (adjust if needed)\n",
    "    base_dir = \"./data\"\n",
    "\n",
    "    # Keywords for classification\n",
    "    category_keywords = {\n",
    "        \"text\": [\"Transcript.csv\"],\n",
    "        \"audio\": [\n",
    "            \"BoAW\", \"OpenSMILE\", \"mfcc\", \"egemaps\", \"densenet\", \"vgg16\"\n",
    "        ],\n",
    "        \"video\": [\n",
    "            \"BoVW\", \"OpenFace\", \"Pose\", \"Gaze\", \"AUs\", \"CNN_ResNet\", \"CNN_VGG\"\n",
    "        ],\n",
    "        \"clinical\": [\"Transcript.csv\"]\n",
    "    }\n",
    "\n",
    "    # Traverse subject folders\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        subject_path = os.path.join(base_dir, folder_name)\n",
    "        print(subject_path)\n",
    "\n",
    "        if not os.path.isdir(subject_path):\n",
    "            print(f\"‚ùå Skipping non-folder: {subject_path}\")\n",
    "            continue\n",
    "\n",
    "        if not folder_name.endswith(\"_P\"):\n",
    "            print(f\"‚ùå Skipping non-subject folder: {subject_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîÑ Processing: {folder_name}\")\n",
    "        subject_id = folder_name.split(\"_\")[0]\n",
    "\n",
    "        # Create subfolders for each category (text, audio, video, clinical)\n",
    "        for cat in category_keywords:\n",
    "            os.makedirs(os.path.join(subject_path, cat), exist_ok=True)\n",
    "\n",
    "        # --- Move transcript ---\n",
    "        transcript_path = os.path.join(subject_path, f\"{subject_id}_Transcript.csv\")\n",
    "        if os.path.exists(transcript_path):\n",
    "            dest_text = os.path.join(subject_path, \"text\", f\"{subject_id}_Transcript.csv\")\n",
    "            dest_clinical = os.path.join(subject_path, \"clinical\", f\"{subject_id}_Transcript.csv\")\n",
    "            shutil.move(transcript_path, dest_text)\n",
    "            shutil.copy(dest_text, dest_clinical)\n",
    "            print(f\"üìÑ Moved and copied transcript to text/ and clinical/\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Transcript not found: {transcript_path}\")\n",
    "\n",
    "        # --- Process feature files from 'features' subfolder ---\n",
    "        features_path = os.path.join(subject_path, \"features\")  # Correct folder path for features\n",
    "        if os.path.exists(features_path):\n",
    "            for file in os.listdir(features_path):\n",
    "                src_file = os.path.join(features_path, file)\n",
    "                moved = False\n",
    "                # Process features based on category keywords\n",
    "                for cat, keywords in category_keywords.items():\n",
    "                    # Skip text and clinical as they are already processed\n",
    "                    if cat in [\"text\", \"clinical\"]:\n",
    "                        continue\n",
    "\n",
    "                    if any(k.lower() in file.lower() for k in keywords):\n",
    "                        dst = os.path.join(subject_path, cat, file)\n",
    "                        shutil.move(src_file, dst)\n",
    "                        print(f\"üì¶ Moved {file} ‚Üí {cat}/\")\n",
    "                        moved = True\n",
    "                        break  # Once moved, exit loop and move on to the next file\n",
    "                \n",
    "                if not moved:\n",
    "                    print(f\"‚ùì Unclassified feature file: {file}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Features folder not found: {features_path}\")\n",
    "\n",
    "        # Clean up empty 'features' folder\n",
    "        if os.path.exists(features_path) and not os.listdir(features_path):\n",
    "            os.rmdir(features_path)\n",
    "            print(f\"üóëÔ∏è Removed empty features/ folder\")\n",
    "\n",
    "        # Clean up empty directories\n",
    "        for cat in category_keywords:\n",
    "            cat_path = os.path.join(subject_path, cat)\n",
    "            if os.path.exists(cat_path) and not os.listdir(cat_path):\n",
    "                os.rmdir(cat_path)\n",
    "                print(f\"üóëÔ∏è Removed empty {cat}/ folder\")\n",
    "\n",
    "    print(\"\\n‚úÖ Done restructuring each *_P folder.\")\n",
    "else:\n",
    "    print(\"‚ùå Operation cancelled. Folder restructuring was not performed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "493e2a32-1f7d-4cc9-9639-5b79393b0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to delete all .wav files in the /data folder?  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deleted 0 .wav files from /data folder.\n"
     ]
    }
   ],
   "source": [
    "# This script deletes all .wav files from the './data' directory after user confirmation.\n",
    "\n",
    "import os\n",
    "\n",
    "# Ask user for confirmation\n",
    "user_input = input(\"Do you want to delete all .wav files in the /data folder? \").strip().lower()\n",
    "\n",
    "if user_input == 'yes':\n",
    "    # Path to the /data directory\n",
    "    data_dir = \"../data\"\n",
    "\n",
    "    # Walk through the /data directory and delete .wav files\n",
    "    deleted_files = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(dirpath, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    deleted_files.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Deleted {len(deleted_files)} .wav files from /data folder.\")\n",
    "else:\n",
    "    print(\"‚ùå Operation cancelled. No .wav files were deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbac5041-5c3d-4293-a587-3ad76245ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure of .:\n",
      "|-- convert_py.ipynb\n",
      "|-- main.ipynb\n",
      "|-- .DS_Store\n",
      "|-- requirements.txt\n",
      "|-- training\n",
      "    |-- train_text.ipynb\n",
      "    |-- train_multimodal.ipynb\n",
      "    |-- train_video.ipynb\n",
      "    |-- train_audio.ipynb\n",
      "|-- utils\n",
      "    |-- metrics.ipynb\n",
      "    |-- __pycache__\n",
      "        |-- feature_validator.cpython-310.pyc\n",
      "    |-- logger.ipynb\n",
      "    |-- helpers.ipynb\n",
      "    |-- config.ipynb\n",
      "    |-- feature_analycis.ipynb\n",
      "    |-- .ipynb_checkpoints\n",
      "        |-- feature_analycis-checkpoint.ipynb\n",
      "        |-- config-checkpoint.ipynb\n",
      "|-- dir_structure_check.ipynb\n",
      "|-- models\n",
      "    |-- audio_model.ipynb\n",
      "    |-- video_model.ipynb\n",
      "    |-- multimodal_fusion.ipynb\n",
      "    |-- text_model.ipynb\n",
      "|-- README.md\n",
      "|-- saved_models\n",
      "    |-- video\n",
      "    |-- .DS_Store\n",
      "    |-- multimodal\n",
      "    |-- audio\n",
      "    |-- text\n",
      "|-- evaluation\n",
      "    |-- evaluate_multimodal.ipynb\n",
      "    |-- evaluate_video.ipynb\n",
      "    |-- evaluate_text.ipynb\n",
      "    |-- evaluate_audio.ipynb\n",
      "|-- .ipynb_checkpoints\n",
      "    |-- requirements-checkpoint.txt\n",
      "    |-- main-checkpoint.ipynb\n",
      "    |-- dir_structure_check-checkpoint.ipynb\n",
      "    |-- folder_setup-checkpoint.ipynb\n",
      "    |-- README-checkpoint.md\n",
      "    |-- convert_py-checkpoint.ipynb\n",
      "|-- preprocessing\n",
      "    |-- align_modalities.ipynb\n",
      "    |-- clinical_preprocess.ipynb\n",
      "    |-- text_preprocess.ipynb\n",
      "    |-- .DS_Store\n",
      "    |-- video_preprocess.ipynb\n",
      "    |-- audio_preprocess.ipynb\n",
      "    |-- .ipynb_checkpoints\n",
      "        |-- clinical_preprocess-checkpoint.ipynb\n",
      "        |-- audio_preprocess-checkpoint.ipynb\n",
      "        |-- video_preprocess-checkpoint.ipynb\n",
      "        |-- text_preprocess-checkpoint.ipynb\n",
      "        |-- align_modalities-checkpoint.ipynb\n",
      "|-- data\n",
      "    |-- 300_P\n",
      "        |-- video\n",
      "            |-- 300_CNN_VGG.mat\n",
      "            |-- 300_CNN_ResNet.mat\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_300_CNN_VGG_feature.csv\n",
      "                |-- processed_300_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "                |-- processed_300_CNN_ResNet_feature.csv\n",
      "                |-- processed_300_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "            |-- 300_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "            |-- 300_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "        |-- .DS_Store\n",
      "        |-- audio\n",
      "            |-- 300_vgg16.csv\n",
      "            |-- 300_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "            |-- 300_OpenSMILE2.3.0_mfcc.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "                |-- 300_BoAW_openSMILE_2.3.0_eGeMAPS-checkpoint.csv\n",
      "                |-- 300_OpenSMILE2.3.0_mfcc-checkpoint.csv\n",
      "            |-- 300_OpenSMILE2.3.0_egemaps.csv\n",
      "            |-- processed\n",
      "                |-- processed_300_vgg16.csv\n",
      "                |-- processed_300_OpenSMILE2.3.0_egemaps.csv\n",
      "                |-- processed_300_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "                |-- processed_300_OpenSMILE2.3.0_mfcc.csv\n",
      "                |-- processed_300_densenet201.csv\n",
      "                |-- processed_300_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "            |-- 300_densenet201.csv\n",
      "            |-- 300_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "        |-- clinical\n",
      "            |-- 300_Transcript_biobert_features.csv\n",
      "            |-- 300_Transcript.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_300_Transcript_biobert_features.csv\n",
      "        |-- text\n",
      "            |-- 300_Transcript_processed_scaled.csv\n",
      "            |-- 300_Transcript.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_300_Transcript_processed_scaled.csv\n",
      "    |-- .DS_Store\n",
      "    |-- lables\n",
      "        |-- .DS_Store\n",
      "        |-- processed\n",
      "            |-- depression_labels.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "                |-- depression_labels-checkpoint.csv\n",
      "        |-- detailed_lables.csv\n",
      "    |-- 302_P\n",
      "        |-- video\n",
      "            |-- 302_CNN_ResNet.mat\n",
      "            |-- 302_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_302_CNN_VGG_feature.csv\n",
      "                |-- processed_302_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "                |-- processed_302_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "                |-- processed_302_CNN_ResNet_feature.csv\n",
      "            |-- 302_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "            |-- 302_CNN_VGG.mat\n",
      "        |-- audio\n",
      "            |-- 302_OpenSMILE2.3.0_egemaps.csv\n",
      "            |-- 302_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "            |-- 302_densenet201.csv\n",
      "            |-- 302_OpenSMILE2.3.0_mfcc.csv\n",
      "            |-- 302_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "            |-- 302_vgg16.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_302_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "                |-- processed_302_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "                |-- processed_302_OpenSMILE2.3.0_mfcc.csv\n",
      "                |-- processed_302_densenet201.csv\n",
      "                |-- processed_302_OpenSMILE2.3.0_egemaps.csv\n",
      "                |-- processed_302_vgg16.csv\n",
      "        |-- clinical\n",
      "            |-- 302_Transcript.csv\n",
      "            |-- 302_Transcript_biobert_features.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_302_Transcript_biobert_features.csv\n",
      "        |-- text\n",
      "            |-- 302_Transcript.csv\n",
      "            |-- 302_Transcript_processed_scaled.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_302_Transcript_processed_scaled.csv\n",
      "    |-- 308_P\n",
      "        |-- video\n",
      "            |-- 308_CNN_ResNet.mat\n",
      "            |-- 308_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "            |-- 308_CNN_VGG.mat\n",
      "            |-- 308_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "            |-- processed\n",
      "                |-- processed_308_CNN_VGG_feature.csv\n",
      "                |-- processed_308_CNN_ResNet_feature.csv\n",
      "                |-- processed_308_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "                |-- processed_308_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "        |-- audio\n",
      "            |-- 308_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "            |-- 308_OpenSMILE2.3.0_egemaps.csv\n",
      "            |-- 308_densenet201.csv\n",
      "            |-- 308_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "            |-- processed\n",
      "                |-- processed_308_vgg16.csv\n",
      "                |-- processed_308_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "                |-- processed_308_densenet201.csv\n",
      "                |-- processed_308_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "                |-- processed_308_OpenSMILE2.3.0_egemaps.csv\n",
      "                |-- processed_308_OpenSMILE2.3.0_mfcc.csv\n",
      "            |-- 308_OpenSMILE2.3.0_mfcc.csv\n",
      "            |-- 308_vgg16.csv\n",
      "        |-- clinical\n",
      "            |-- 308_Transcript_biobert_features.csv\n",
      "            |-- 308_Transcript.csv\n",
      "            |-- processed\n",
      "                |-- processed_308_Transcript_biobert_features.csv\n",
      "        |-- text\n",
      "            |-- 308_Transcript_processed_scaled.csv\n",
      "            |-- 308_Transcript.csv\n",
      "            |-- processed\n",
      "                |-- processed_308_Transcript_processed_scaled.csv\n",
      "    |-- aligned_multimodal_dataset.csv\n",
      "    |-- 301_P\n",
      "        |-- video\n",
      "            |-- 301_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "            |-- 301_CNN_ResNet.mat\n",
      "            |-- 301_CNN_VGG.mat\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_301_OpenFace2.1.0_Pose_gaze_AUs.csv\n",
      "                |-- processed_301_CNN_VGG_feature.csv\n",
      "                |-- processed_301_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "                |-- processed_301_CNN_ResNet_feature.csv\n",
      "            |-- 301_BoVW_openFace_2.1.0_Pose_Gaze_AUs.csv\n",
      "        |-- audio\n",
      "            |-- 301_OpenSMILE2.3.0_mfcc.csv\n",
      "            |-- 301_vgg16.csv\n",
      "            |-- 301_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "            |-- 301_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "            |-- 301_densenet201.csv\n",
      "            |-- 301_OpenSMILE2.3.0_egemaps.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "                |-- 301_BoAW_openSMILE_2.3.0_eGeMAPS-checkpoint.csv\n",
      "            |-- processed\n",
      "                |-- processed_301_OpenSMILE2.3.0_egemaps.csv\n",
      "                |-- processed_301_OpenSMILE2.3.0_mfcc.csv\n",
      "                |-- processed_301_vgg16.csv\n",
      "                |-- processed_301_BoAW_openSMILE_2.3.0_eGeMAPS.csv\n",
      "                |-- processed_301_densenet201.csv\n",
      "                |-- processed_301_BoAW_openSMILE_2.3.0_MFCC.csv\n",
      "        |-- clinical\n",
      "            |-- 301_Transcript.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- processed\n",
      "                |-- processed_301_Transcript_biobert_features.csv\n",
      "            |-- 301_Transcript_biobert_features.csv\n",
      "        |-- text\n",
      "            |-- 301_Transcript.csv\n",
      "            |-- .ipynb_checkpoints\n",
      "            |-- 301_Transcript_processed_scaled.csv\n",
      "            |-- processed\n",
      "                |-- processed_301_Transcript_processed_scaled.csv\n",
      "    |-- .ipynb_checkpoints\n",
      "        |-- aligned_multimodal_dataset-checkpoint.csv\n",
      "|-- notebooks\n",
      "|-- folder_setup.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Use this to check the folder structure\n",
    "import os\n",
    "\n",
    "def print_tree(directory, level=0):\n",
    "    # List all files and directories in the current directory\n",
    "    with os.scandir(directory) as entries:\n",
    "        for entry in entries:\n",
    "            # Indentation for tree structure\n",
    "            print(' ' * (level * 4) + '|--', entry.name)\n",
    "            # Recursively call the function for directories\n",
    "            if entry.is_dir():\n",
    "                print_tree(entry.path, level + 1)\n",
    "\n",
    "# Specify the root directory of your project\n",
    "root_directory = '.'  # Change this path to your project directory\n",
    "\n",
    "print(f\"Directory structure of {root_directory}:\")\n",
    "print_tree(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbd248-c8ca-4189-b878-d992959301d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script converts all Jupyter notebooks (except itself) to .py scripts after user confirmation.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Ask user for confirmation\n",
    "user_input = input(\"Do you want to convert all .ipynb files (excluding this script) to .py? If yes, type 'yes': \").strip().lower()\n",
    "\n",
    "if user_input == 'yes':\n",
    "    # Set root directory to start from the location of the current notebook\n",
    "    root_dir = Path('.').resolve()\n",
    "\n",
    "    # Folders to exclude\n",
    "    exclude_folders = {'data'}\n",
    "\n",
    "    # Function to convert notebook to .py\n",
    "    def convert_ipynb_to_py(ipynb_path):\n",
    "        with open(ipynb_path, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "        exporter = PythonExporter()\n",
    "        source_code, _ = exporter.from_notebook_node(notebook)\n",
    "\n",
    "        py_path = ipynb_path.with_suffix('.py')\n",
    "        with open(py_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(source_code)\n",
    "        print(f\"‚úÖ Converted: {ipynb_path.relative_to(root_dir)} ‚Üí {py_path.relative_to(root_dir)}\")\n",
    "\n",
    "    # Recursively walk through directories\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Skip excluded folders\n",
    "        if any(excluded in Path(dirpath).parts for excluded in exclude_folders):\n",
    "            continue\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.ipynb') and filename != 'convert_py.ipynb':\n",
    "                file_path = Path(dirpath) / filename\n",
    "                convert_ipynb_to_py(file_path)\n",
    "else:\n",
    "    print(\"‚ùå Operation cancelled. No notebooks were converted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b37662-bca8-4f70-a178-f4f9ec433f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script deletes .py files that were generated from .ipynb files after user confirmation.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ask user for confirmation\n",
    "user_input = input(\"Do you want to delete all .py files corresponding to .ipynb notebooks? If yes, type 'yes': \").strip().lower()\n",
    "\n",
    "if user_input == 'yes':\n",
    "    # Set root directory to current notebook's location\n",
    "    root_dir = Path('.').resolve()\n",
    "    exclude_folders = {'data'}\n",
    "\n",
    "    # Function to delete .py file corresponding to .ipynb file\n",
    "    def delete_py_file(ipynb_path):\n",
    "        py_path = ipynb_path.with_suffix('.py')\n",
    "        if py_path.exists():\n",
    "            py_path.unlink()\n",
    "            print(f\"üóëÔ∏è Deleted: {py_path.relative_to(root_dir)}\")\n",
    "\n",
    "    # Recursively walk through directories\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        # Skip excluded folders\n",
    "        if any(excluded in Path(dirpath).parts for excluded in exclude_folders):\n",
    "            continue\n",
    "\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.ipynb') and filename != 'convert_py.ipynb':\n",
    "                file_path = Path(dirpath) / filename\n",
    "                delete_py_file(file_path)\n",
    "else:\n",
    "    print(\"‚ùå Operation cancelled. No .py files were deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
