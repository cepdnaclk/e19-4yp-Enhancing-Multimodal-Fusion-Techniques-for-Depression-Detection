{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8630fa03-9678-4589-9f93-1358b50f7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading RoBERTa model...\n",
      "Found 3 users to process\n",
      "Processing 302_P (0.0MB)\n",
      "Extracting features for 99 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved raw RoBERTa features for 302_P (99 samples)\n",
      "Processing 301_P (0.0MB)\n",
      "Extracting features for 72 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved raw RoBERTa features for 301_P (72 samples)\n",
      "⚠️  No transcript file found for .ipynb_checkpoints\n",
      "\n",
      "📊 Processing complete:\n",
      "✅ Successful: 2\n",
      "❌ Failed: 1\n",
      "📁 Total users: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Key Features:\n",
    "- Iterates through user folders in the raw data path.\n",
    "- Reads transcript CSV files containing text data.\n",
    "- Uses Hugging Face's RoBERTa model to extract [CLS] token embeddings for each text segment.\n",
    "- Handles empty texts, missing files, and batch processing errors gracefully.\n",
    "- Saves the extracted 768-dimensional RoBERTa features into `.parquet` files.\n",
    "- Preserves additional timing information (Start_Time, End_Time) if available.\n",
    "- Feature columns renamed to text_f1, text_f2, ..., text_f768\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RAW_DATA_PATH = '../../data/raw'\n",
    "SAVE_BASE_PATH = '../../data/interim/text_features'\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading RoBERTa model...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_roberta_embeddings(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Extract RoBERTa embeddings from texts with proper memory management\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([]).reshape(0, 768)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch = [text if text and text.strip() else \"[EMPTY]\" for text in batch]\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_embeddings)\n",
    "            \n",
    "            del inputs, outputs\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            zero_embeddings = np.zeros((len(batch), 768))\n",
    "            embeddings.append(zero_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_user_data(user_path, save_dir, user_name):\n",
    "    \"\"\"Process a single user's data\"\"\"\n",
    "    transcript_file = next(\n",
    "        (f for f in os.listdir(user_path) if f.endswith('_Transcript.csv')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if transcript_file is None:\n",
    "        print(f\"⚠️  No transcript file found for {user_name}\")\n",
    "        return False\n",
    "    \n",
    "    csv_path = os.path.join(user_path, transcript_file)\n",
    "    \n",
    "    try:\n",
    "        file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "        print(f\"Processing {user_name} ({file_size_mb:.1f}MB)\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if 'Text' not in df.columns:\n",
    "            print(f\"⚠️  No 'Text' column found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        texts = df['Text'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        if not texts:\n",
    "            print(f\"⚠️  No text data found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Extracting features for {len(texts)} texts...\")\n",
    "        features = get_roberta_embeddings(texts)\n",
    "        \n",
    "        if features.size == 0:\n",
    "            print(f\"⚠️  No features extracted for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Rename feature columns to text_f1, text_f2, ..., text_f768\n",
    "        df_features = pd.DataFrame(\n",
    "            features, \n",
    "            columns=[f'text_f{i+1}' for i in range(features.shape[1])]\n",
    "        )\n",
    "        \n",
    "        # Add Start_Time and End_Time if available\n",
    "        if 'Start_Time' in df.columns:\n",
    "            df_features['Start_Time'] = df['Start_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"⚠️  No 'Start_Time' column found for {user_name}\")\n",
    "            \n",
    "        if 'End_Time' in df.columns:\n",
    "            df_features['End_Time'] = df['End_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"⚠️  No 'End_Time' column found for {user_name}\")\n",
    "        \n",
    "        # Save to parquet\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        output_path = os.path.join(save_dir, 'text_features.parquet')\n",
    "        df_features.to_parquet(output_path, index=False)\n",
    "        \n",
    "        print(f\"✅ Saved raw RoBERTa features for {user_name} ({features.shape[0]} samples)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {user_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(RAW_DATA_PATH):\n",
    "        print(f\"❌ Raw data path does not exist: {RAW_DATA_PATH}\")\n",
    "        return\n",
    "    \n",
    "    users = [u for u in os.listdir(RAW_DATA_PATH) \n",
    "             if os.path.isdir(os.path.join(RAW_DATA_PATH, u))]\n",
    "    \n",
    "    if not users:\n",
    "        print(\"❌ No user directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(users)} users to process\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for user in users:\n",
    "        user_path = os.path.join(RAW_DATA_PATH, user, 'text')\n",
    "        \n",
    "        if not os.path.isdir(user_path):\n",
    "            print(f\"⚠️  Text directory not found for {user}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "        \n",
    "        save_dir = os.path.join(SAVE_BASE_PATH, user)\n",
    "        \n",
    "        if process_user_data(user_path, save_dir, user):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\n📊 Processing complete:\")\n",
    "    print(f\"✅ Successful: {successful}\")\n",
    "    print(f\"❌ Failed: {failed}\")\n",
    "    print(f\"📁 Total users: {len(users)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a01ff8-bd69-4394-95c4-d390f1245831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
