{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0690cf79-e6ac-4845-9692-210ee6124dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking modality: audio_features (../../data/interim/audio_features)\n",
      "üìÅ 301_P | üìÑ processed_audio_features.parquet --> Shape: (8239, 101) (rows, columns) | Time columns: ['time']\n",
      "üìÅ 301_P | üìÑ processed_audio_features02.parquet --> Shape: (8239, 101) (rows, columns) | Time columns: ['time']\n",
      "üìÅ 302_P | üìÑ processed_audio_features.parquet --> Shape: (7575, 101) (rows, columns) | Time columns: ['time']\n",
      "üìÅ 302_P | üìÑ processed_audio_features02.parquet --> Shape: (7575, 101) (rows, columns) | Time columns: ['time']\n",
      "\n",
      "üîç Checking modality: clinical_features (../../data/interim/clinical_features)\n",
      "üìÅ 301_P | üìÑ clinical_features.parquet --> Shape: (72, 770) (rows, columns) | Time columns: ['End_Time', 'Start_Time']\n",
      "üìÅ 302_P | üìÑ clinical_features.parquet --> Shape: (99, 770) (rows, columns) | Time columns: ['End_Time', 'Start_Time']\n",
      "\n",
      "üîç Checking modality: text_features (../../data/interim/text_features)\n",
      "üìÅ 301_P | üìÑ text_features.parquet --> Shape: (72, 770) (rows, columns) | Time columns: ['End_Time', 'Start_Time']\n",
      "üìÅ 302_P | üìÑ text_features.parquet --> Shape: (99, 770) (rows, columns) | Time columns: ['End_Time', 'Start_Time']\n",
      "\n",
      "üîç Checking modality: video_features (../../data/interim/video_features)\n",
      "üìÅ 301_P | üìÑ processed_video_features01.parquet --> Shape: (8240, 101) (rows, columns) | Time columns: ['time']\n",
      "üìÅ 302_P | üìÑ processed_video_features01.parquet --> Shape: (7588, 101) (rows, columns) | Time columns: ['time']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base paths for all modalities\n",
    "BASE_DIR = '../../data/interim/'  # Assuming you're running from 'interim' folder\n",
    "MODALITY_FOLDERS = ['audio_features', 'clinical_features', 'text_features', 'video_features']\n",
    "\n",
    "def check_parquet_files_shapes_and_columns(modality_path, modality_name):\n",
    "    print(f\"\\nüîç Checking modality: {modality_name} ({modality_path})\")\n",
    "\n",
    "    time_columns_to_check = {'time', 'Start_Time', 'End_Time'}\n",
    "\n",
    "    for user_folder in sorted(os.listdir(modality_path)):\n",
    "        user_path = os.path.join(modality_path, user_folder)\n",
    "        if not os.path.isdir(user_path) or user_folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        for file in sorted(os.listdir(user_path)):\n",
    "            if file.endswith('.parquet'):\n",
    "                file_path = os.path.join(user_path, file)\n",
    "                try:\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                    shape_str = f\"Shape: {df.shape} (rows, columns)\"\n",
    "\n",
    "                    # Check if any of the time columns exist\n",
    "                    existing_time_cols = [col for col in time_columns_to_check if col in df.columns]\n",
    "\n",
    "                    if existing_time_cols:\n",
    "                        print(f\"üìÅ {user_folder} | üìÑ {file} --> {shape_str} | Time columns: {existing_time_cols}\")\n",
    "                    else:\n",
    "                        print(f\"üìÅ {user_folder} | üìÑ {file} --> {shape_str}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "\n",
    "# Main loop to check all modalities\n",
    "for modality in MODALITY_FOLDERS:\n",
    "    modality_path = os.path.join(BASE_DIR, modality)\n",
    "    if os.path.exists(modality_path):\n",
    "        check_parquet_files_shapes_and_columns(modality_path, modality)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Folder not found: {modality_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5a34298-77b7-4374-ab9e-a2b8760473c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ User: 301_P\n",
      "Combined shape: (72, 1838)\n",
      "Saved combined data to: ../../data/processed/301_P/combined_features.parquet\n",
      "\n",
      "‚úÖ User: 302_P\n",
      "Combined shape: (99, 1838)\n",
      "Saved combined data to: ../../data/processed/302_P/combined_features.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Base directories\n",
    "BASE_DIR = '../../data/interim'\n",
    "OUTPUT_ROOT = '../../data/processed'\n",
    "\n",
    "MODALITY_PATHS = {\n",
    "    'clinical': os.path.join(BASE_DIR, 'clinical_features'),\n",
    "    'text': os.path.join(BASE_DIR, 'text_features'),\n",
    "    'video': os.path.join(BASE_DIR, 'video_features'),\n",
    "    'audio': os.path.join(BASE_DIR, 'audio_features'),\n",
    "}\n",
    "\n",
    "FILE_NAMES = {\n",
    "    'clinical': 'clinical_features.parquet',\n",
    "    'text': 'text_features.parquet',\n",
    "    'video': 'processed_video_features01.parquet',\n",
    "    'audio': ['processed_audio_features.parquet', 'processed_audio_features02.parquet'],\n",
    "}\n",
    "\n",
    "def average_pool(df, intervals_df):\n",
    "    pooled_rows = []\n",
    "    for _, interval in intervals_df.iterrows():\n",
    "        mask = (df['time'] >= interval['Start_Time']) & (df['time'] <= interval['End_Time'])\n",
    "        window_df = df.loc[mask].drop(columns=['time'], errors='ignore')\n",
    "        if len(window_df) > 0:\n",
    "            pooled_rows.append(window_df.mean(numeric_only=True))\n",
    "        else:\n",
    "            pooled_rows.append(pd.Series([float('nan')] * (df.shape[1]-1), index=df.columns.drop('time')))\n",
    "    return pd.DataFrame(pooled_rows)\n",
    "\n",
    "def combine_all_modalities(user_id):\n",
    "    clinical_path = os.path.join(MODALITY_PATHS['clinical'], user_id, FILE_NAMES['clinical'])\n",
    "    text_path = os.path.join(MODALITY_PATHS['text'], user_id, FILE_NAMES['text'])\n",
    "    video_path = os.path.join(MODALITY_PATHS['video'], user_id, FILE_NAMES['video'])\n",
    "    audio_paths = [os.path.join(MODALITY_PATHS['audio'], user_id, f) for f in FILE_NAMES['audio']]\n",
    "\n",
    "    # Check existence\n",
    "    if not os.path.isfile(clinical_path):\n",
    "        print(f\"‚ö†Ô∏è Missing clinical file: {clinical_path}\")\n",
    "        return\n",
    "    if not os.path.isfile(text_path):\n",
    "        print(f\"‚ö†Ô∏è Missing text file: {text_path}\")\n",
    "        return\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"‚ö†Ô∏è Missing video file: {video_path}\")\n",
    "        return\n",
    "    for p in audio_paths:\n",
    "        if not os.path.isfile(p):\n",
    "            print(f\"‚ö†Ô∏è Missing audio file: {p}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        clinical_df = pd.read_parquet(clinical_path)\n",
    "        text_df = pd.read_parquet(text_path)\n",
    "        video_df = pd.read_parquet(video_path)\n",
    "        audio_dfs = [pd.read_parquet(p) for p in audio_paths]\n",
    "\n",
    "        # Check rows alignment\n",
    "        if clinical_df.shape[0] != text_df.shape[0]:\n",
    "            print(f\"‚ùå Row count mismatch for user {user_id}: clinical {clinical_df.shape[0]}, text {text_df.shape[0]}\")\n",
    "            return\n",
    "\n",
    "        interval_cols = ['Start_Time', 'End_Time']\n",
    "        intervals_df = clinical_df[interval_cols].copy()\n",
    "\n",
    "        clinical_features = clinical_df.drop(columns=interval_cols)\n",
    "        text_features = text_df.drop(columns=interval_cols)\n",
    "\n",
    "        pooled_video_df = average_pool(video_df, intervals_df)\n",
    "        pooled_audio_dfs = [average_pool(df, intervals_df) for df in audio_dfs]\n",
    "        pooled_audio_df = pd.concat(pooled_audio_dfs, axis=1)\n",
    "\n",
    "        combined_df = pd.concat([intervals_df, clinical_features, text_features, pooled_video_df, pooled_audio_df], axis=1)\n",
    "\n",
    "        # Save combined dataframe\n",
    "        user_output_dir = os.path.join(OUTPUT_ROOT, user_id)\n",
    "        os.makedirs(user_output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(user_output_dir, 'combined_features.parquet')\n",
    "        combined_df.to_parquet(output_path)\n",
    "\n",
    "        print(f\"\\n‚úÖ User: {user_id}\")\n",
    "        print(f\"Combined shape: {combined_df.shape}\")\n",
    "        print(f\"Saved combined data to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing user {user_id}: {e}\")\n",
    "\n",
    "# Get list of users (assuming same users in clinical folder)\n",
    "user_folders = sorted(os.listdir(MODALITY_PATHS['clinical']))\n",
    "user_folders = [u for u in user_folders if os.path.isdir(os.path.join(MODALITY_PATHS['clinical'], u)) and not u.startswith('.')]\n",
    "\n",
    "for user_id in user_folders:\n",
    "    combine_all_modalities(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "119b455c-9df9-4fa8-96c4-9f0940b73bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä User: 301_P\n",
      "Shape: (72, 1838)\n",
      "No columns contain NULL values.\n",
      "Time-related columns found: ['Start_Time', 'End_Time']\n",
      "\n",
      "üìä User: 302_P\n",
      "Shape: (99, 1838)\n",
      "No columns contain NULL values.\n",
      "Time-related columns found: ['Start_Time', 'End_Time']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_DIR = '../../data/processed'\n",
    "\n",
    "def analyze_combined_file(user_id):\n",
    "    file_path = os.path.join(PROCESSED_DIR, user_id, 'combined_features.parquet')\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"‚ö†Ô∏è File not found for user {user_id}: {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        print(f\"\\nüìä User: {user_id}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "\n",
    "        # Null columns\n",
    "        null_cols = df.columns[df.isnull().any()].tolist()\n",
    "        if null_cols:\n",
    "            print(f\"Columns with NULL values: {null_cols}\")\n",
    "        else:\n",
    "            print(\"No columns contain NULL values.\")\n",
    "\n",
    "        # Time columns detection (case-insensitive)\n",
    "        time_cols = [col for col in df.columns if 'time' in col.lower()]\n",
    "        if time_cols:\n",
    "            print(f\"Time-related columns found: {time_cols}\")\n",
    "        else:\n",
    "            print(\"No time-related columns found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading/analyzing user {user_id}: {e}\")\n",
    "\n",
    "# List all users based on folders in processed directory\n",
    "user_folders = sorted(os.listdir(PROCESSED_DIR))\n",
    "user_folders = [u for u in user_folders if os.path.isdir(os.path.join(PROCESSED_DIR, u)) and not u.startswith('.')]\n",
    "\n",
    "for user_id in user_folders:\n",
    "    analyze_combined_file(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad4d5ef1-4223-4cfc-bb5b-3c72c34a2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Summary of all users' feature files:\n",
      " User  File Size (MB)  Rows  Columns\n",
      "301_P           1.553    72     1838\n",
      "302_P           1.765    99     1838\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_DIR = '../../data/processed'\n",
    "\n",
    "def get_file_info(user_id):\n",
    "    file_path = os.path.join(PROCESSED_DIR, user_id, 'combined_features.parquet')\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"‚ö†Ô∏è File not found for user {user_id}: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # size in MB\n",
    "        rows, cols = df.shape\n",
    "        return {\n",
    "            'User': user_id,\n",
    "            'File Size (MB)': round(file_size_mb, 3),\n",
    "            'Rows': rows,\n",
    "            'Columns': cols\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file for user {user_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Collect info for all users\n",
    "user_folders = sorted(os.listdir(PROCESSED_DIR))\n",
    "user_folders = [u for u in user_folders if os.path.isdir(os.path.join(PROCESSED_DIR, u)) and not u.startswith('.')]\n",
    "\n",
    "all_info = []\n",
    "\n",
    "for user_id in user_folders:\n",
    "    info = get_file_info(user_id)\n",
    "    if info:\n",
    "        all_info.append(info)\n",
    "\n",
    "# Print combined table\n",
    "if all_info:\n",
    "    summary_df = pd.DataFrame(all_info)\n",
    "    print(\"\\nüìã Summary of all users' feature files:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No user files found or readable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83cd249f-c8e4-4545-ac78-6d8a9f34dc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded data for user: 301_P | Shape: (72, 1839) | File size: 1.55 MB\n",
      "‚úÖ Loaded data for user: 302_P | Shape: (99, 1839) | File size: 1.76 MB\n",
      "\n",
      "üîó Combined data shape: (171, 1839)\n",
      "üíæ Saved combined data to: ../../data/processed/all_user_combined_data.parquet | File size: 2.36 MB\n",
      "\n",
      "üìã Preview of combined data (first 10 rows, columns shown: 10)\n",
      " Start_Time  End_Time user_id  clinical_f0  clinical_f1  clinical_f2  clinical_f3  clinical_f4  clinical_f5  clinical_f6\n",
      "        0.8       7.0   301_P     0.406030     0.313151    -0.116788     0.061381    -0.090267    -0.005268     0.106040\n",
      "       41.9      42.5   301_P     0.233028     0.043382    -0.064205     0.025771    -0.487324     0.013373     0.252081\n",
      "       52.9      55.8   301_P     0.344104     0.117400    -0.099952    -0.063165    -0.242215     0.155228     0.353929\n",
      "       59.7      60.7   301_P     0.474687     0.313247    -0.061940    -0.060701    -0.246777    -0.055384     0.218618\n",
      "       63.4      64.2   301_P     0.428376     0.279073    -0.105950    -0.099083    -0.233661     0.009878     0.144821\n",
      "       67.3      69.9   301_P     0.342122    -0.089282    -0.387762     0.054438    -0.027567     0.200241     0.150800\n",
      "       73.5      75.1   301_P     0.597304     0.204454     0.108131     0.200350    -0.227972     0.000928     0.271013\n",
      "       78.1      79.6   301_P     0.412884     0.122717    -0.154558    -0.052275    -0.324499     0.399393     0.050039\n",
      "       85.2      89.5   301_P     0.346278     0.433228    -0.289390     0.174352    -0.002657    -0.062813     0.011987\n",
      "       95.3      98.4   301_P     0.175885    -0.023508    -0.224614    -0.272035    -0.537383     0.297654     0.161185\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_DIR = '../../data/processed'\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DIR, 'all_user_combined_data.parquet')\n",
    "\n",
    "def load_and_combine_all_users(base_dir):\n",
    "    user_folders = sorted([\n",
    "        u for u in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, u)) and not u.startswith('.')\n",
    "    ])\n",
    "\n",
    "    all_dfs = []\n",
    "    for user_id in user_folders:\n",
    "        file_path = os.path.join(base_dir, user_id, 'combined_features.parquet')\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found for user {user_id}: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            df['user_id'] = user_id  # Add user_id column\n",
    "            all_dfs.append(df)\n",
    "            print(f\"‚úÖ Loaded data for user: {user_id} | Shape: {df.shape} | File size: {file_size_mb:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data for user {user_id}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n",
    "        print(f\"\\nüîó Combined data shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå No data loaded. Combined dataframe is empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preview_combined_data(df, max_cols=10, max_rows=10):\n",
    "    # Always include user_id and time-related columns (case-insensitive)\n",
    "    must_include = [col for col in df.columns if col == 'user_id' or 'time' in col.lower()]\n",
    "    # Other columns to fill the rest of max_cols (excluding must_include)\n",
    "    other_cols = [col for col in df.columns if col not in must_include]\n",
    "    \n",
    "    # Limit number of other columns to max_cols - len(must_include)\n",
    "    n_other = max(0, max_cols - len(must_include))\n",
    "    selected_cols = must_include + other_cols[:n_other]\n",
    "\n",
    "    print(f\"\\nüìã Preview of combined data (first {max_rows} rows, columns shown: {len(selected_cols)})\")\n",
    "    print(df[selected_cols].head(max_rows).to_string(index=False))\n",
    "\n",
    "def main():\n",
    "    combined_data = load_and_combine_all_users(PROCESSED_DIR)\n",
    "    if not combined_data.empty:\n",
    "        combined_data.to_parquet(OUTPUT_FILE, index=False)\n",
    "        final_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "        print(f\"üíæ Saved combined data to: {OUTPUT_FILE} | File size: {final_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Print preview with required columns\n",
    "        preview_combined_data(combined_data, max_cols=10, max_rows=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aad5893f-dac4-45c0-b94b-e1d6a4bcd495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in ../../data/labels/\n",
      "Detailed_PHQ8_Labels.csv\n",
      ".ipynb_checkpoints\n",
      "detailed_lables.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '../../data/labels/'\n",
    "\n",
    "files = os.listdir(file_path)\n",
    "print(\"Files in\", file_path)\n",
    "for f in files:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d567838-d16b-4fd4-9b80-aba7f1c66a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Participant  age gender  Depression_label split\n",
      "         300   33   male                 0   dev\n",
      "         301   39   male                 0   dev\n",
      "         302   25   male                 0 train\n",
      "         303   41 female                 0 train\n",
      "         304   22 female                 0 train\n",
      "         305   55   male                 0 train\n",
      "         306   62 female                 0   dev\n",
      "         307   23 female                 0 train\n",
      "         308   40 female                 1 train\n",
      "         309   19   male                 1 train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../../data/labels/detailed_lables.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the desired columns\n",
    "selected_cols = ['Participant', 'age', 'gender', 'Depression_label','split']\n",
    "df_selected = df[selected_cols]\n",
    "\n",
    "# Show the first 10 rows as a table\n",
    "print(df_selected.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "078da86d-bb59-4462-84b9-90b346ce7e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded data for user: 301_P | Shape: (72, 1839) | File size: 1.55 MB\n",
      "‚úÖ Loaded data for user: 302_P | Shape: (99, 1839) | File size: 1.76 MB\n",
      "\n",
      "üîó Combined data shape: (171, 1839)\n",
      "üíæ Saved final merged data to: ../../data/processed/all_user_combined_data.parquet | File size: 2.36 MB\n",
      "\n",
      "üìã Preview of data (first 100 rows, columns shown: 10)\n",
      " age gender  Depression_label split user_id  Start_Time  End_Time  clinical_f0  clinical_f1  clinical_f2\n",
      "  39   male                 0   dev   301_P         0.8       7.0     0.406030     0.313151    -0.116788\n",
      "  39   male                 0   dev   301_P        41.9      42.5     0.233028     0.043382    -0.064205\n",
      "  39   male                 0   dev   301_P        52.9      55.8     0.344104     0.117400    -0.099952\n",
      "  39   male                 0   dev   301_P        59.7      60.7     0.474687     0.313247    -0.061940\n",
      "  39   male                 0   dev   301_P        63.4      64.2     0.428376     0.279073    -0.105950\n",
      "  39   male                 0   dev   301_P        67.3      69.9     0.342122    -0.089282    -0.387762\n",
      "  39   male                 0   dev   301_P        73.5      75.1     0.597304     0.204454     0.108131\n",
      "  39   male                 0   dev   301_P        78.1      79.6     0.412884     0.122717    -0.154558\n",
      "  39   male                 0   dev   301_P        85.2      89.5     0.346278     0.433228    -0.289390\n",
      "  39   male                 0   dev   301_P        95.3      98.4     0.175885    -0.023508    -0.224614\n",
      "  39   male                 0   dev   301_P        99.6     107.7     0.403821     0.240312    -0.061783\n",
      "  39   male                 0   dev   301_P       112.1     113.0     0.298472    -0.026202    -0.287693\n",
      "  39   male                 0   dev   301_P       115.9     122.5     0.183112     0.350192    -0.416779\n",
      "  39   male                 0   dev   301_P       126.5     134.8     0.347121     0.301970    -0.329585\n",
      "  39   male                 0   dev   301_P       141.9     145.6     0.554046     0.304303    -0.233418\n",
      "  39   male                 0   dev   301_P       146.6     154.5     0.235718     0.278217    -0.254841\n",
      "  39   male                 0   dev   301_P       161.3     165.3     0.264845     0.271103    -0.327569\n",
      "  39   male                 0   dev   301_P       167.9     181.0     0.182819     0.219299    -0.317238\n",
      "  39   male                 0   dev   301_P       188.8     206.1     0.185284     0.283458    -0.059236\n",
      "  39   male                 0   dev   301_P       210.9     211.5     0.539667    -0.138622    -0.384517\n",
      "  39   male                 0   dev   301_P       213.5     217.9     0.262184    -0.026112    -0.026772\n",
      "  39   male                 0   dev   301_P       219.3     224.5     0.325102     0.356479    -0.014991\n",
      "  39   male                 0   dev   301_P       227.2     234.8     0.373107     0.555242     0.154431\n",
      "  39   male                 0   dev   301_P       241.8     244.7     0.394725     0.462648    -0.019847\n",
      "  39   male                 0   dev   301_P       257.3     259.9     0.446709     0.364223    -0.159654\n",
      "  39   male                 0   dev   301_P       261.0     266.0     0.313242     0.475906    -0.278611\n",
      "  39   male                 0   dev   301_P       268.9     278.5     0.325155     0.164363    -0.105129\n",
      "  39   male                 0   dev   301_P       282.9     285.2     0.627268     0.277061     0.070370\n",
      "  39   male                 0   dev   301_P       287.0     312.1     0.132117     0.246259    -0.329552\n",
      "  39   male                 0   dev   301_P       322.5     334.2     0.028449     0.270628    -0.110998\n",
      "  39   male                 0   dev   301_P       339.5     340.1     0.564147     0.171156     0.322785\n",
      "  39   male                 0   dev   301_P       344.3     346.2     0.233028     0.043382    -0.064205\n",
      "  39   male                 0   dev   301_P       349.3     354.1     0.117181     0.388202    -0.015945\n",
      "  39   male                 0   dev   301_P       355.5     373.4     0.307480     0.175197    -0.375060\n",
      "  39   male                 0   dev   301_P       381.1     382.4     0.292783     0.314013    -0.129852\n",
      "  39   male                 0   dev   301_P       384.0     392.2     0.098209     0.167990    -0.238061\n",
      "  39   male                 0   dev   301_P       393.2     409.7     0.184413     0.398259    -0.256009\n",
      "  39   male                 0   dev   301_P       410.8     416.7     0.363380     0.305983    -0.253301\n",
      "  39   male                 0   dev   301_P       418.6     443.0     0.147021     0.175239    -0.189817\n",
      "  39   male                 0   dev   301_P       451.8     455.5     0.282387     0.070930    -0.055382\n",
      "  39   male                 0   dev   301_P       457.8     469.2     0.405138     0.444131    -0.152195\n",
      "  39   male                 0   dev   301_P       473.1     495.1     0.218747    -0.018674    -0.399297\n",
      "  39   male                 0   dev   301_P       503.1     504.6     0.293932     0.002700    -0.060812\n",
      "  39   male                 0   dev   301_P       508.0     508.5     0.293932     0.002700    -0.060812\n",
      "  39   male                 0   dev   301_P       511.1     511.6     0.139034    -0.181277    -0.093978\n",
      "  39   male                 0   dev   301_P       520.4     540.1     0.494885     0.223615    -0.318637\n",
      "  39   male                 0   dev   301_P       541.1     559.8     0.275623     0.248908    -0.723702\n",
      "  39   male                 0   dev   301_P       563.8     564.2     0.368522    -0.354121    -0.600095\n",
      "  39   male                 0   dev   301_P       567.6     568.9     0.329283     0.120116    -0.109240\n",
      "  39   male                 0   dev   301_P       569.8     595.3    -0.029406     0.419373    -0.567221\n",
      "  39   male                 0   dev   301_P       596.3     598.6     0.287045     0.474348    -0.137353\n",
      "  39   male                 0   dev   301_P       601.9     631.7     0.197905     0.293401    -0.234608\n",
      "  39   male                 0   dev   301_P       642.2     644.2     0.149622    -0.131145    -0.013025\n",
      "  39   male                 0   dev   301_P       650.4     665.9     0.263123     0.174450    -0.159727\n",
      "  39   male                 0   dev   301_P       666.9     688.3     0.088234     0.343093    -0.270510\n",
      "  39   male                 0   dev   301_P       689.3     690.5     0.375977     0.159512    -0.245881\n",
      "  39   male                 0   dev   301_P       691.5     700.2     0.355077     0.155832    -0.110699\n",
      "  39   male                 0   dev   301_P       704.6     705.0     0.368522    -0.354121    -0.600095\n",
      "  39   male                 0   dev   301_P       716.1     718.4     0.288724     0.323722    -0.287217\n",
      "  39   male                 0   dev   301_P       722.4     725.9     0.377729     0.319356    -0.063330\n",
      "  39   male                 0   dev   301_P       726.9     732.2     0.361392     0.211766    -0.336246\n",
      "  39   male                 0   dev   301_P       733.7     745.2     0.326110     0.150729    -0.155289\n",
      "  39   male                 0   dev   301_P       746.6     753.8     0.166120     0.263781    -0.246117\n",
      "  39   male                 0   dev   301_P       760.8     766.7     0.308684     0.107896    -0.022089\n",
      "  39   male                 0   dev   301_P       767.5     773.0     0.340032     0.365989    -0.178567\n",
      "  39   male                 0   dev   301_P       774.6     783.5     0.260451     0.360389    -0.282947\n",
      "  39   male                 0   dev   301_P       784.7     790.0     0.440484     0.273685     0.017101\n",
      "  39   male                 0   dev   301_P       797.8     798.3     0.233028     0.043382    -0.064205\n",
      "  39   male                 0   dev   301_P       799.0     801.4     0.170191    -0.076082    -0.170017\n",
      "  39   male                 0   dev   301_P       802.2     802.8     0.415859     0.160778     0.007424\n",
      "  39   male                 0   dev   301_P       818.0     819.2     0.288273    -0.101193     0.071584\n",
      "  39   male                 0   dev   301_P         0.8     822.9     0.337363     0.058442    -0.146780\n",
      "  25   male                 0 train   302_P         2.1       3.2     0.611645     0.464170     0.006225\n",
      "  25   male                 0 train   302_P        26.3      27.1     0.185062     0.208745    -0.157247\n",
      "  25   male                 0 train   302_P        58.2      59.1     0.454683     0.206740     0.178161\n",
      "  25   male                 0 train   302_P        59.6      61.0     0.284531     0.537625    -0.020100\n",
      "  25   male                 0 train   302_P        66.6      67.3     0.417610     0.249552     0.074050\n",
      "  25   male                 0 train   302_P        69.0      71.2     0.408229    -0.288391    -0.491958\n",
      "  25   male                 0 train   302_P        75.0      75.8     0.535310     0.369001     0.089125\n",
      "  25   male                 0 train   302_P        79.8      80.3     0.233027     0.043382    -0.064205\n",
      "  25   male                 0 train   302_P        84.3      88.5     0.396173     0.226247    -0.382696\n",
      "  25   male                 0 train   302_P        89.4      92.1     0.394000     0.136909    -0.042614\n",
      "  25   male                 0 train   302_P       100.2     101.9     0.388533    -0.110607    -0.015515\n",
      "  25   male                 0 train   302_P       106.8     109.8     0.187643     0.304339    -0.181818\n",
      "  25   male                 0 train   302_P       113.9     116.2     0.171366     0.389960    -0.099083\n",
      "  25   male                 0 train   302_P       123.1     131.3     0.164956     0.346282    -0.215468\n",
      "  25   male                 0 train   302_P       134.3     134.9     0.548207     0.255980     0.329722\n",
      "  25   male                 0 train   302_P       137.4     140.6     0.335422     0.006865    -0.537616\n",
      "  25   male                 0 train   302_P       145.7     150.9     0.166649     0.341748    -0.152603\n",
      "  25   male                 0 train   302_P       157.6     158.6     0.125575     0.186327     0.067122\n",
      "  25   male                 0 train   302_P       161.5     162.6     0.149838     0.207344    -0.247684\n",
      "  25   male                 0 train   302_P       168.1     173.5     0.253242     0.111511    -0.424760\n",
      "  25   male                 0 train   302_P       175.6     182.1     0.134191     0.160388    -0.628817\n",
      "  25   male                 0 train   302_P       192.8     193.2     0.139034    -0.181277    -0.093978\n",
      "  25   male                 0 train   302_P       196.7     203.3     0.406096     0.329075    -0.406924\n",
      "  25   male                 0 train   302_P       212.1     212.7     0.582642    -0.378173    -0.425443\n",
      "  25   male                 0 train   302_P       218.2     219.0     0.247029     0.312303    -0.030126\n",
      "  25   male                 0 train   302_P       229.8     232.0     0.503908     0.401200    -0.513871\n",
      "  25   male                 0 train   302_P       234.0     235.0     0.558809     0.205701    -0.038193\n",
      "  25   male                 0 train   302_P       242.0     249.3     0.234400     0.265894    -0.485541\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROCESSED_DIR = '../../data/processed'\n",
    "LABELS_FILE = '../../data/labels/detailed_lables.csv'\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DIR, 'all_user_combined_data.parquet')\n",
    "\n",
    "def load_and_combine_all_users(base_dir):\n",
    "    user_folders = sorted([\n",
    "        u for u in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, u)) and not u.startswith('.')\n",
    "    ])\n",
    "\n",
    "    all_dfs = []\n",
    "    for user_id in user_folders:\n",
    "        file_path = os.path.join(base_dir, user_id, 'combined_features.parquet')\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found for user {user_id}: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            df['user_id'] = user_id  # Add user_id column\n",
    "            all_dfs.append(df)\n",
    "            print(f\"‚úÖ Loaded data for user: {user_id} | Shape: {df.shape} | File size: {file_size_mb:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data for user {user_id}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n",
    "        print(f\"\\nüîó Combined data shape: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå No data loaded. Combined dataframe is empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preview_data(df, max_cols=15, max_rows=10):\n",
    "    # Priority columns to show first if they exist\n",
    "    metadata_cols = ['age', 'gender', 'Depression_label', 'split']\n",
    "    metadata_present = [col for col in metadata_cols if col in df.columns]\n",
    "\n",
    "    user_time_cols = []\n",
    "    if 'user_id' in df.columns:\n",
    "        user_time_cols.append('user_id')\n",
    "    user_time_cols += [col for col in df.columns if 'time' in col.lower()]\n",
    "\n",
    "    # Other columns excluding the prioritized ones\n",
    "    other_cols = [col for col in df.columns if col not in metadata_present + user_time_cols]\n",
    "\n",
    "    # Limit number of other columns shown\n",
    "    n_other = max(0, max_cols - len(metadata_present) - len(user_time_cols))\n",
    "\n",
    "    selected_cols = metadata_present + user_time_cols + other_cols[:n_other]\n",
    "\n",
    "    print(f\"\\nüìã Preview of data (first {max_rows} rows, columns shown: {len(selected_cols)})\")\n",
    "    print(df[selected_cols].head(max_rows).to_string(index=False))\n",
    "\n",
    "def main():\n",
    "    combined_data = load_and_combine_all_users(PROCESSED_DIR)\n",
    "    if combined_data.empty:\n",
    "        return\n",
    "\n",
    "    # Load labels\n",
    "    labels_df = pd.read_csv(LABELS_FILE)\n",
    "    selected_cols = ['Participant', 'age', 'gender', 'Depression_label', 'split']\n",
    "    labels_df = labels_df[selected_cols]\n",
    "\n",
    "    # Prepare for merge: convert user_id like '301_P' to int Participant '301'\n",
    "    combined_data['Participant'] = combined_data['user_id'].str.replace('_P', '').astype(int)\n",
    "\n",
    "    # Merge on Participant\n",
    "    merged_df = combined_data.merge(labels_df, on='Participant', how='left')\n",
    "\n",
    "    # Drop Participant column after merge\n",
    "    merged_df = merged_df.drop(columns=['Participant'])\n",
    "\n",
    "    # Save merged dataframe\n",
    "    merged_df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    final_size_mb = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "    print(f\"üíæ Saved final merged data to: {OUTPUT_FILE} | File size: {final_size_mb:.2f} MB\")\n",
    "\n",
    "    # Preview merged data with desired columns shown first\n",
    "    preview_data(merged_df, max_cols=10, max_rows=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0606de-8b4e-4ceb-83eb-49d7710b17a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
