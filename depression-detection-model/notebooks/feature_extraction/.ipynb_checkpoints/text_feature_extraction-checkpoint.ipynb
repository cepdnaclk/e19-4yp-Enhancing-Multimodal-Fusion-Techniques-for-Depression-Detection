{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8630fa03-9678-4589-9f93-1358b50f7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/depression-nlp-fyp/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/depression-nlp-fyp/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading RoBERTa model...\n",
      "Found 3 users to process\n",
      "Processing 302_P (0.0MB)\n",
      "Extracting features for 99 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved RoBERTa features for 302_P (99 samples)\n",
      "Processing 301_P (0.0MB)\n",
      "Extracting features for 72 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:03<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved RoBERTa features for 301_P (72 samples)\n",
      "âš ï¸  No transcript file found for .ipynb_checkpoints\n",
      "\n",
      "ðŸ“Š Processing complete:\n",
      "âœ… Successful: 2\n",
      "âŒ Failed: 1\n",
      "ðŸ“ Total users: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Key Features:\n",
    "- Iterates through user folders in the raw data path.\n",
    "- Reads transcript CSV files containing text data.\n",
    "- Uses Hugging Face's RoBERTa model to extract [CLS] token embeddings for each text segment.\n",
    "- Handles empty texts, missing files, and batch processing errors gracefully.\n",
    "- Saves the extracted 768-dimensional RoBERTa features into `.parquet` files.\n",
    "- Preserves additional timing information (Start_Time, End_Time) if available.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RAW_DATA_PATH = '../../data/raw'\n",
    "SAVE_BASE_PATH = '../../data/interim/text_features'\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading RoBERTa model...\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_roberta_embeddings(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Extract RoBERTa embeddings from texts with proper memory management\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([]).reshape(0, 768)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Handle empty strings\n",
    "        batch = [text if text and text.strip() else \"[EMPTY]\" for text in batch]\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_embeddings)\n",
    "            \n",
    "            # Clear memory\n",
    "            del inputs, outputs\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Create zero embeddings for failed batch\n",
    "            batch_size_actual = len(batch)\n",
    "            zero_embeddings = np.zeros((batch_size_actual, 768))\n",
    "            embeddings.append(zero_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_user_data(user_path, save_dir, user_name):\n",
    "    \"\"\"Process a single user's data\"\"\"\n",
    "    transcript_file = next(\n",
    "        (f for f in os.listdir(user_path) if f.endswith('_Transcript.csv')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if transcript_file is None:\n",
    "        print(f\"âš ï¸  No transcript file found for {user_name}\")\n",
    "        return False\n",
    "    \n",
    "    csv_path = os.path.join(user_path, transcript_file)\n",
    "    \n",
    "    try:\n",
    "        # Check file size\n",
    "        file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "        print(f\"Processing {user_name} ({file_size_mb:.1f}MB)\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if 'Text' not in df.columns:\n",
    "            print(f\"âš ï¸  No 'Text' column found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Get texts and handle NaN values\n",
    "        texts = df['Text'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        if not texts:\n",
    "            print(f\"âš ï¸  No text data found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Extracting features for {len(texts)} texts...\")\n",
    "        features = get_roberta_embeddings(texts)\n",
    "        \n",
    "        if features.size == 0:\n",
    "            print(f\"âš ï¸  No features extracted for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Create DataFrame with features\n",
    "        df_features = pd.DataFrame(\n",
    "            features, \n",
    "            columns=[f'roberta_{i}' for i in range(768)]\n",
    "        )\n",
    "        \n",
    "        # Add Start_Time and End_Time if they exist in the original data\n",
    "        if 'Start_Time' in df.columns:\n",
    "            df_features['Start_Time'] = df['Start_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"âš ï¸  No 'Start_Time' column found for {user_name}\")\n",
    "            \n",
    "        if 'End_Time' in df.columns:\n",
    "            df_features['End_Time'] = df['End_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"âš ï¸  No 'End_Time' column found for {user_name}\")\n",
    "        \n",
    "        # Save to parquet\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        output_path = os.path.join(save_dir, 'text_features.parquet')\n",
    "        df_features.to_parquet(output_path, index=False)\n",
    "        \n",
    "        print(f\"âœ… Saved RoBERTa features for {user_name} ({features.shape[0]} samples)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {user_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(RAW_DATA_PATH):\n",
    "        print(f\"âŒ Raw data path does not exist: {RAW_DATA_PATH}\")\n",
    "        return\n",
    "    \n",
    "    users = [u for u in os.listdir(RAW_DATA_PATH) \n",
    "             if os.path.isdir(os.path.join(RAW_DATA_PATH, u))]\n",
    "    \n",
    "    if not users:\n",
    "        print(\"âŒ No user directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(users)} users to process\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for user in users:\n",
    "        user_path = os.path.join(RAW_DATA_PATH, user, 'text')\n",
    "        \n",
    "        if not os.path.isdir(user_path):\n",
    "            print(f\"âš ï¸  Text directory not found for {user}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "        \n",
    "        save_dir = os.path.join(SAVE_BASE_PATH, user)\n",
    "        \n",
    "        if process_user_data(user_path, save_dir, user):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Processing complete:\")\n",
    "    print(f\"âœ… Successful: {successful}\")\n",
    "    print(f\"âŒ Failed: {failed}\")\n",
    "    print(f\"ðŸ“ Total users: {len(users)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f2ff62-52be-476c-84a9-48555474d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (99, 770)\n",
      "Total columns: 770\n",
      "\n",
      "First 5 columns: ['roberta_0', 'roberta_1', 'roberta_2', 'roberta_3', 'roberta_4']\n",
      "Last 5 columns: ['roberta_765', 'roberta_766', 'roberta_767', 'Start_Time', 'End_Time']\n",
      "\n",
      "Table (first 5 rows, first 5 + last 5 columns):\n",
      "   roberta_0  roberta_1  roberta_2  roberta_3  roberta_4  roberta_765  \\\n",
      "0  -0.064926   0.078818  -0.011347  -0.113043   0.029975    -0.069268   \n",
      "1  -0.042070   0.085485  -0.017387  -0.116804   0.060461    -0.036051   \n",
      "2  -0.049929   0.104638  -0.014817  -0.095213   0.051695    -0.067631   \n",
      "3  -0.066134   0.092893  -0.003128  -0.094649   0.026224    -0.039669   \n",
      "4  -0.039353   0.088576  -0.026071  -0.093649   0.039769    -0.059562   \n",
      "\n",
      "   roberta_766  roberta_767  Start_Time  End_Time  \n",
      "0    -0.069701    -0.008505         2.1       3.2  \n",
      "1    -0.030257    -0.027823        26.3      27.1  \n",
      "2    -0.045495    -0.002897        58.2      59.1  \n",
      "3    -0.043128    -0.031936        59.6      61.0  \n",
      "4    -0.019522    -0.029947        66.6      67.3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to saved features\n",
    "FEATURES_BASE_PATH = '../../data/interim/text_features'\n",
    "user_id = '302_P'  # Change this to your user ID\n",
    "\n",
    "# Load the data\n",
    "file_path = os.path.join(FEATURES_BASE_PATH, user_id, 'text_features.parquet')\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Select first 5 and last 5 columns\n",
    "first_5_cols = df.columns[:5].tolist()\n",
    "last_5_cols = df.columns[-5:].tolist()\n",
    "selected_cols = first_5_cols + last_5_cols\n",
    "\n",
    "# Create a subset with selected columns and first 5 rows\n",
    "subset = df[selected_cols].head(5)\n",
    "\n",
    "print(f\"\\nFirst 5 columns: {first_5_cols}\")\n",
    "print(f\"Last 5 columns: {last_5_cols}\")\n",
    "\n",
    "print(f\"\\nTable (first 5 rows, first 5 + last 5 columns):\")\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe5f0c-b831-46e5-8a12-d75305f930ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "SAVE_BASE_PATH = '../../data/interim/text_features'\n",
    "\n",
    "def load_all_features(base_path):\n",
    "    \"\"\"Load all user feature parquet files and concatenate them.\"\"\"\n",
    "    all_features = []\n",
    "    for user in os.listdir(base_path):\n",
    "        user_path = os.path.join(base_path, user, 'text_features.parquet')\n",
    "        if os.path.isfile(user_path):\n",
    "            df = pd.read_parquet(user_path)\n",
    "            # Select only RoBERTa feature columns (768 dims)\n",
    "            feature_cols = [col for col in df.columns if col.startswith('roberta_')]\n",
    "            all_features.append(df[feature_cols])\n",
    "        else:\n",
    "            print(f\"âš ï¸ No features file for user {user}\")\n",
    "    return pd.concat(all_features, ignore_index=True)\n",
    "\n",
    "def plot_explained_variance(pca):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance (%)')\n",
    "    plt.title('Explained Variance by PCA Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_2d(pca_data, labels=None, title='PCA 2D projection'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(pca_data[:, 0], pca_data[:, 1], c=labels, cmap='viridis', alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Label')\n",
    "    else:\n",
    "        plt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.7)\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"Loading all RoBERTa features...\")\n",
    "    features = load_all_features(SAVE_BASE_PATH)\n",
    "    print(f\"Loaded features shape: {features.shape}\")\n",
    "    \n",
    "    \n",
    "    features_np = features.values  # convert to numpy\n",
    "    \n",
    "    print(\"Applying PCA...\")\n",
    "    pca = PCA(n_components=50)  # or n_components=0.95 for variance threshold\n",
    "    pca_result = pca.fit_transform(features_np)\n",
    "    \n",
    "    print(f\"Explained variance by 50 components: {np.sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "    plot_explained_variance(pca)\n",
    "    \n",
    "    # Visualize first 2 principal components\n",
    "    visualize_2d(pca_result[:, :2])\n",
    "    \n",
    "    pca_df = pd.DataFrame(\n",
    "    pca_result, \n",
    "    columns=[f'pc_{i+1}' for i in range(pca_result.shape[1])]\n",
    "    )\n",
    "    \n",
    "    pca_df.to_parquet(os.path.join(SAVE_BASE_PATH, 'pca_features.parquet'), index=False)\n",
    "\n",
    "print(f\"âœ… Saved PCA features with shape {pca_df.shape} to {SAVE_BASE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
