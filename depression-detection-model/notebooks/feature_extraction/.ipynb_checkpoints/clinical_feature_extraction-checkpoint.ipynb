{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43daada5-4055-4d6d-83db-ad01a15d2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Using device: cpu\n",
      "🔬 Loading BioBERT model...\n",
      "👥 Found 3 users to process\n",
      "\n",
      "🩺 Processing 302_P (0.0MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Extracting batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clinical features for 302_P (99 samples)\n",
      "\n",
      "🩺 Processing 301_P (0.0MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Extracting batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:02<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clinical features for 301_P (72 samples)\n",
      "⚠️ No clinical transcript found for .ipynb_checkpoints\n",
      "\n",
      "📊 Clinical Feature Extraction Summary:\n",
      "✅ Successful: 2\n",
      "❌ Failed: 1\n",
      "📁 Total: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script extracts [CLS] embeddings from clinical transcripts using BioBERT.\n",
    "Saves 768-d embeddings (with time info if present) for each user separately.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress logs and warnings\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RAW_DATA_PATH = '../../data/raw'\n",
    "SAVE_BASE_PATH = '../../data/interim/clinical_features'\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Choose BioBERT variant from Hugging Face\n",
    "BIoBERT_MODEL = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🧠 Using device: {device}\")\n",
    "\n",
    "print(\"🔬 Loading BioBERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BIoBERT_MODEL)\n",
    "model = AutoModel.from_pretrained(BIoBERT_MODEL)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_biobert_embeddings(texts, batch_size=BATCH_SIZE):\n",
    "    if not texts:\n",
    "        return np.array([]).reshape(0, 768)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔄 Extracting batches\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch = [text if text and text.strip() else \"[EMPTY]\" for text in batch]\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch, \n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_embeddings)\n",
    "            \n",
    "            del inputs, outputs\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Batch {i//batch_size + 1} failed: {e}\")\n",
    "            zero_embeddings = np.zeros((len(batch), 768))\n",
    "            embeddings.append(zero_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_user_clinical(user_path, save_dir, user_name):\n",
    "    transcript_file = next(\n",
    "        (f for f in os.listdir(user_path) if f.endswith('_Transcript.csv')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if transcript_file is None:\n",
    "        print(f\"⚠️ No clinical transcript found for {user_name}\")\n",
    "        return False\n",
    "\n",
    "    csv_path = os.path.join(user_path, transcript_file)\n",
    "    \n",
    "    try:\n",
    "        file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "        print(f\"\\n🩺 Processing {user_name} ({file_size_mb:.1f}MB)\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if 'Text' not in df.columns:\n",
    "            print(f\"⚠️ 'Text' column missing in {user_name}'s file\")\n",
    "            return False\n",
    "        \n",
    "        texts = df['Text'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        if not texts:\n",
    "            print(f\"⚠️ No clinical text for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        features = get_biobert_embeddings(texts)\n",
    "        if features.size == 0:\n",
    "            print(f\"⚠️ No embeddings for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        df_features = pd.DataFrame(features, columns=[f'biobert_{i}' for i in range(768)])\n",
    "        \n",
    "        if 'Start_Time' in df.columns:\n",
    "            df_features['Start_Time'] = df['Start_Time'].reset_index(drop=True)\n",
    "        if 'End_Time' in df.columns:\n",
    "            df_features['End_Time'] = df['End_Time'].reset_index(drop=True)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        output_path = os.path.join(save_dir, 'clinical_features.parquet')\n",
    "        df_features.to_parquet(output_path, index=False)\n",
    "        \n",
    "        print(f\"✅ Saved clinical features for {user_name} ({features.shape[0]} samples)\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {user_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(RAW_DATA_PATH):\n",
    "        print(f\"❌ Raw data path does not exist: {RAW_DATA_PATH}\")\n",
    "        return\n",
    "    \n",
    "    users = [u for u in os.listdir(RAW_DATA_PATH) \n",
    "             if os.path.isdir(os.path.join(RAW_DATA_PATH, u))]\n",
    "    \n",
    "    if not users:\n",
    "        print(\"❌ No user directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"👥 Found {len(users)} users to process\")\n",
    "    success, failure = 0, 0\n",
    "    \n",
    "    for user in users:\n",
    "        user_path = os.path.join(RAW_DATA_PATH, user, 'clinical')\n",
    "        if not os.path.isdir(user_path):\n",
    "            print(f\"⚠️ Clinical directory not found for {user}\")\n",
    "            failure += 1\n",
    "            continue\n",
    "        \n",
    "        save_dir = os.path.join(SAVE_BASE_PATH, user)\n",
    "        if process_user_clinical(user_path, save_dir, user):\n",
    "            success += 1\n",
    "        else:\n",
    "            failure += 1\n",
    "    \n",
    "    print(f\"\\n📊 Clinical Feature Extraction Summary:\")\n",
    "    print(f\"✅ Successful: {success}\")\n",
    "    print(f\"❌ Failed: {failure}\")\n",
    "    print(f\"📁 Total: {len(users)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
