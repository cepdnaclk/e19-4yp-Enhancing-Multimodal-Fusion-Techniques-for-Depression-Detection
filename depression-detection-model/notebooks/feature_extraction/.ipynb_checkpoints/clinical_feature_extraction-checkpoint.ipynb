{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43daada5-4055-4d6d-83db-ad01a15d2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clinicalbert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m MODEL_OPTIONS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiobert\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Choose your model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_OPTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclinicalbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Change this as needed\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clinicalbert'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, logging\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress warnings\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setup\n",
    "RAW_DATA_PATH = '../../data/raw'\n",
    "SAVE_BASE_PATH = '../../data/interim/clinical'\n",
    "BATCH_SIZE = 8  # Reduced for CPU processing\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_OPTIONS = {\n",
    "    'biobert': 'dmis-lab/biobert-base-cased-v1.1',\n",
    "}\n",
    "\n",
    "# Choose your model\n",
    "MODEL_NAME = MODEL_OPTIONS['biobert']\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Successfully loaded {MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load {MODEL_NAME}: {e}\")\n",
    "    # Fallback to BERT base\n",
    "    MODEL_NAME = MODEL_OPTIONS['bert_base']\n",
    "    print(f\"Falling back to {MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Extract BERT embeddings from texts with proper memory management\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([]).reshape(0, 768)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Handle empty strings\n",
    "        batch = [text if text and text.strip() else \"[EMPTY]\" for text in batch]\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(cls_embeddings)\n",
    "            \n",
    "            # Clear memory\n",
    "            del inputs, outputs\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "            # Create zero embeddings for failed batch\n",
    "            batch_size_actual = len(batch)\n",
    "            embedding_dim = 768  # Standard BERT embedding dimension\n",
    "            zero_embeddings = np.zeros((batch_size_actual, embedding_dim))\n",
    "            embeddings.append(zero_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_user_data(user_path, save_dir, user_name):\n",
    "    \"\"\"Process a single user's data\"\"\"\n",
    "    transcript_file = next(\n",
    "        (f for f in os.listdir(user_path) if f.endswith('_Transcript.csv')), \n",
    "        None\n",
    "    )\n",
    "    \n",
    "    if transcript_file is None:\n",
    "        print(f\"‚ö†Ô∏è  No transcript file found for {user_name}\")\n",
    "        return False\n",
    "    \n",
    "    csv_path = os.path.join(user_path, transcript_file)\n",
    "    \n",
    "    try:\n",
    "        # Check file size\n",
    "        file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "        print(f\"Processing {user_name} ({file_size_mb:.1f}MB)\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if 'Text' not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  No 'Text' column found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Get texts and handle NaN values\n",
    "        texts = df['Text'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        if not texts:\n",
    "            print(f\"‚ö†Ô∏è  No text data found for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Extracting features for {len(texts)} texts...\")\n",
    "        features = get_bert_embeddings(texts)\n",
    "        \n",
    "        if features.size == 0:\n",
    "            print(f\"‚ö†Ô∏è  No features extracted for {user_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Create DataFrame with features\n",
    "        feature_prefix = MODEL_NAME.split('/')[-1].replace('-', '_')\n",
    "        df_features = pd.DataFrame(\n",
    "            features, \n",
    "            columns=[f'{feature_prefix}_{i}' for i in range(features.shape[1])]\n",
    "        )\n",
    "        \n",
    "        # Add Start_Time and End_Time if they exist in the original data\n",
    "        if 'Start_Time' in df.columns:\n",
    "            df_features['Start_Time'] = df['Start_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No 'Start_Time' column found for {user_name}\")\n",
    "            \n",
    "        if 'End_Time' in df.columns:\n",
    "            df_features['End_Time'] = df['End_Time'].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No 'End_Time' column found for {user_name}\")\n",
    "        \n",
    "        # Save to parquet\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        output_path = os.path.join(save_dir, f'{feature_prefix}_features.parquet')\n",
    "        df_features.to_parquet(output_path, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Saved {feature_prefix} features for {user_name} ({features.shape[0]} samples)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {user_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main processing loop\n",
    "def main():\n",
    "    if not os.path.exists(RAW_DATA_PATH):\n",
    "        print(f\"‚ùå Raw data path does not exist: {RAW_DATA_PATH}\")\n",
    "        return\n",
    "    \n",
    "    users = [u for u in os.listdir(RAW_DATA_PATH) \n",
    "             if os.path.isdir(os.path.join(RAW_DATA_PATH, u))]\n",
    "    \n",
    "    if not users:\n",
    "        print(\"‚ùå No user directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(users)} users to process\")\n",
    "    print(f\"Using model: {MODEL_NAME}\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for user in users:\n",
    "        user_path = os.path.join(RAW_DATA_PATH, user, 'text')\n",
    "        \n",
    "        if not os.path.isdir(user_path):\n",
    "            print(f\"‚ö†Ô∏è  Text directory not found for {user}\")\n",
    "            failed += 1\n",
    "            continue\n",
    "        \n",
    "        save_dir = os.path.join(SAVE_BASE_PATH, user)\n",
    "        \n",
    "        if process_user_data(user_path, save_dir, user):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nüìä Processing complete:\")\n",
    "    print(f\"‚úÖ Successful: {successful}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"üìÅ Total users: {len(users)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dac7d-98ab-4978-a726-60ce266a1ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
